import sys
import os
import json
from datetime import datetime
import argparse
from pathlib import Path
from pydantic import BaseModel, Field
import litellm
from gepa import optimize
from gepa.core.adapter import EvaluationBatch, GEPAAdapter
from gepa.logging.logger import StdOutLogger
from typing import TypedDict, Any, Protocol
from utils.humaneval_utils import extract_code_from_text, execute_code
from data import init_dataset
import contextlib
# -----------------------------
# HumanEval adapter
# -----------------------------
class HumanEvalDataInst(TypedDict):
    input: str
    additional_context: dict[str, str]
    answer: str

class HumanEvalTrajectory(TypedDict):
    data: HumanEvalDataInst
    full_assistant_response: str

class HumanEvalRolloutOutput(TypedDict):
    full_assistant_response: str

class HumanEvalStructuredOutput(BaseModel):
    code_solution: str = Field(
        ..., description="The complete Python function implementation that solves the given problem"
    )
    explanation: str = Field(
        ..., description="A brief explanation of the approach used to solve the problem"
    )

class HumanEvalAdapter(GEPAAdapter[HumanEvalDataInst, HumanEvalTrajectory, HumanEvalRolloutOutput]):
    """
    HumanEval ë°ì´í„°ì…‹ì„ ìœ„í•œ GEPA ì–´ëŒ‘í„° êµ¬í˜„.
    
    GEPA í”„ë ˆì„ì›Œí¬ë¥¼ ì‚¬ìš©í•˜ì—¬ HumanEval ì½”ë“œ ìƒì„± ë¬¸ì œì— ëŒ€í•œ í”„ë¡¬í”„íŠ¸ ìµœì í™”ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    """
    
    def __init__(self, model: str, failure_score: float = 0.0, api_key: str | None = None):
        """
        Args:
            model: í‰ê°€ì— ì‚¬ìš©í•  LLM ëª¨ë¸ëª…
            failure_score: ì‹¤íŒ¨í•œ ì¼€ì´ìŠ¤ì— ë¶€ì—¬í•  ì ìˆ˜ (ê¸°ë³¸ê°’: 0.0)
            api_key: LLM API í‚¤
        """
        self.model = model
        self.failure_score = failure_score
        self.api_key = api_key

    def _call_llm(self, messages: list[dict], timeout: int = 30) -> str:
        """LLM í˜¸ì¶œì„ ìœ„í•œ ê³µí†µ í•¨ìˆ˜"""
        try:
            response = litellm.completion(
                model=self.model,
                messages=messages,
                api_key=self.api_key,
                timeout=timeout
            )
            return response.choices[0].message.content
        except Exception as e:
            return f"Error: Failed to get response from model - {str(e)}"

    def evaluate(
        self,
        batch: list[HumanEvalDataInst],
        candidate: dict[str, str],
        capture_traces: bool = True,
    ) -> EvaluationBatch[HumanEvalTrajectory, HumanEvalRolloutOutput]:
        """
        HumanEval ë°°ì¹˜ì— ëŒ€í•´ í›„ë³´ í”„ë¡œê·¸ë¨ì„ í‰ê°€í•©ë‹ˆë‹¤.
        
        Args:
            batch: í‰ê°€í•  HumanEval ë°ì´í„° ì¸ìŠ¤í„´ìŠ¤ ëª©ë¡
            candidate: í‰ê°€í•  í›„ë³´ í”„ë¡œê·¸ë¨ (ì»´í¬ë„ŒíŠ¸ëª… -> ì»´í¬ë„ŒíŠ¸ í…ìŠ¤íŠ¸)
            capture_traces: Trueì¸ ê²½ìš° ìƒì„¸í•œ ì¶”ì  ì •ë³´ë¥¼ ìº¡ì²˜
            
        Returns:
            í‰ê°€ ê²°ê³¼ë¥¼ í¬í•¨í•œ EvaluationBatch
            
        Note:
            - ê°œë³„ ì˜ˆì œ ì‹¤íŒ¨ ì‹œ ì˜ˆì™¸ë¥¼ ë°œìƒì‹œí‚¤ì§€ ì•Šê³  failure_scoreë¥¼ ë°˜í™˜
            - ì‹œìŠ¤í…œì  ì‹¤íŒ¨(ëª¨ë¸ ì—†ìŒ, ì„¤ì • ì˜¤ë¥˜ ë“±)ì—ë§Œ ì˜ˆì™¸ ë°œìƒ
        """

        outputs, scores, trajectories = [], [], [] if capture_traces else None

        if not candidate:
            raise ValueError("Candidate must contain at least one component text.")

        system_content = candidate.get("instruction_prompt", "")
        for data in batch:
            messages = [
                {"role": "system", "content": system_content},
                {"role": "user", "content": data['input']}
            ]

            raw_text = self._call_llm(messages)

            entry_point = data["additional_context"].get("entry_point", "")
            code_solution = extract_code_from_text(raw_text, entry_point)

            output_text = f"Assistant output:\n{raw_text}\n\nExtracted code:\n{code_solution}"

            test_cases = data["additional_context"].get("test_cases", "")

            # ë””ë²„ê¹… ì •ë³´ ì¶”ê°€
            debug_info = f"\nDebug Info:\n- Entry point: {entry_point}\n- Code extracted: {len(code_solution) > 0}\n- Test cases available: {len(test_cases) > 0}"

            if code_solution and test_cases and entry_point:
                # í•¨ìˆ˜ ì‹œê·¸ë‹ˆì²˜ ì¶”ì¶œ (promptì—ì„œ)
                import re
                prompt = data['input']
                func_signature_match = re.search(r'def\s+\w+\([^)]*\)\s*->\s*[^:]+:', prompt)
                if not func_signature_match:
                    func_signature_match = re.search(r'def\s+\w+\([^)]*\):', prompt)
                function_signature = func_signature_match.group(0) if func_signature_match else None
                
                score, err_msg, _, _ = execute_code(code_solution, test_cases, entry_point, function_signature=function_signature)
                debug_info += f"\n- Execution score: {score}\n- Error message: {err_msg}"
                if score < 1.0:
                    output_text += f"\nExecution Result: {err_msg}"
            else:
                score = self.failure_score
                if not code_solution:
                    output_text += "\nError: No code could be extracted from response"
                elif not test_cases:
                    output_text += "\nError: No test cases available"
                elif not entry_point:
                    output_text += "\nError: No entry point specified"
            
            output_text += debug_info

            outputs.append({"full_assistant_response": output_text})
            scores.append(score)

            if capture_traces:
                trajectories.append({"data": data, "full_assistant_response": output_text})

        return EvaluationBatch(outputs=outputs, scores=scores, trajectories=trajectories)

    def make_reflective_dataset(
        self,
        candidate: dict[str, str],
        eval_batch: EvaluationBatch[HumanEvalTrajectory, HumanEvalRolloutOutput],
        components_to_update: list[str],
    ) -> dict[str, list[dict[str, Any]]]:
        """
        ë°˜ì„± ë°ì´í„°ì…‹ì„ êµ¬ì¶•í•˜ì—¬ instruction refinementë¥¼ ìœ„í•œ ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        
        Args:
            candidate: evaluate()ì—ì„œ í‰ê°€ëœ ë™ì¼í•œ í›„ë³´ í”„ë¡œê·¸ë¨
            eval_batch: evaluate(..., capture_traces=True)ì˜ ê²°ê³¼
            components_to_update: ì—…ë°ì´íŠ¸í•  ì»´í¬ë„ŒíŠ¸ ëª©ë¡
            
        Returns:
            ì»´í¬ë„ŒíŠ¸ëª… -> ë°˜ì„± ë°ì´í„°ì…‹ ë ˆì½”ë“œ ëª©ë¡ ë§¤í•‘
            ê° ë ˆì½”ë“œëŠ” JSON ì§ë ¬í™” ê°€ëŠ¥í•˜ë©° ë‹¤ìŒ ìŠ¤í‚¤ë§ˆë¥¼ ë”°ë¦…ë‹ˆë‹¤:
            {
                "Inputs": str,           # ì»´í¬ë„ŒíŠ¸ ì…ë ¥ì˜ ìµœì†Œí•œì˜ ê¹”ë”í•œ ë·°
                "Generated Outputs": str, # ëª¨ë¸ ì¶œë ¥ ë˜ëŠ” ì›ì‹œ í…ìŠ¤íŠ¸
                "Feedback": str          # ì„±ëŠ¥ì— ëŒ€í•œ í”¼ë“œë°± (ì •ë‹µ, ì˜¤ë¥˜ ë©”ì‹œì§€ ë“±)
            }
            
        Note:
            - ë°˜ì„± ë°ì´í„°ì…‹ì€ teacher LLMì´ ê°œì„ ëœ ì»´í¬ë„ŒíŠ¸ í…ìŠ¤íŠ¸ë¥¼ ì œì•ˆí•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤
            - ì„±ê³µ/ì‹¤íŒ¨ ì¼€ì´ìŠ¤ ëª¨ë‘ì— ëŒ€í•´ êµ¬ì²´ì ì´ê³  í•™ìŠµ ì¤‘ì‹¬ì ì¸ í”¼ë“œë°±ì„ ì œê³µí•©ë‹ˆë‹¤
        """
        ret_d: dict[str, list[dict[str, Any]]] = {}

        assert len(components_to_update) == 1
        comp = components_to_update[0]

        items: list[dict[str, Any]] = []
        trace_instances = list(zip(eval_batch.trajectories, eval_batch.scores, eval_batch.outputs, strict=False))

        for trace_instance in trace_instances:
            traj, score, _ = trace_instance
            data = traj["data"]
            generated_outputs = traj["full_assistant_response"]

            if score > 0.0:
                # ì„±ê³µ ì¼€ì´ìŠ¤: ë” êµ¬ì²´ì ì¸ í”¼ë“œë°± ì œê³µ
                feedback = self._generate_success_feedback(data, generated_outputs)
            else:
                # ì‹¤íŒ¨ ì¼€ì´ìŠ¤: ìƒì„¸í•œ ì˜¤ë¥˜ ë¶„ì„ê³¼ ê°œì„  ë°©í–¥ ì œì‹œ
                feedback = self._generate_failure_feedback(data, generated_outputs)

            d = {"Inputs": data["input"], "Generated Outputs": generated_outputs, "Feedback": feedback}
            items.append(d)

        ret_d[comp] = items

        if len(items) == 0:
            raise Exception("No valid predictions found for any module.")

        return ret_d

    def _generate_success_feedback(self, data: dict, generated_outputs: str) -> str:
        """ì„±ê³µ ì¼€ì´ìŠ¤ì— ëŒ€í•œ êµ¬ì²´ì ì¸ í”¼ë“œë°± ìƒì„±"""
        entry_point = data["additional_context"].get("entry_point", "")
        
        feedback_parts = [
            "âœ… SUCCESS: The generated code solution is correct and passes all test cases.",
            f"âœ… Function '{entry_point}' was implemented correctly.",
            "âœ… All assertions in the test cases passed.",
            "",
            "ğŸ’¡ Key strengths of this solution:",
            "- Correct function signature and naming",
            "- Proper logic implementation", 
            "- Handles all edge cases correctly",
            "",
            "ğŸ¯ This is a good example of how to approach similar problems."
        ]
        
        return "\n".join(feedback_parts)

    def _generate_failure_feedback(self, data: dict, generated_outputs: str) -> str:
        """ì‹¤íŒ¨ ì¼€ì´ìŠ¤ì— ëŒ€í•œ ìƒì„¸í•œ í”¼ë“œë°± ìƒì„±"""
        test_cases = data["additional_context"].get("test_cases", "")
        entry_point = data["additional_context"].get("entry_point", "")
        canonical_solution = data["additional_context"].get("canonical_solution", "")
        
        # ì½”ë“œ ì¶”ì¶œ ì‹œë„
        from utils.humaneval_utils import extract_code_from_text
        extracted_code = extract_code_from_text(generated_outputs, entry_point)
        
        feedback_parts = [
            "âŒ FAILURE: The generated code solution is incorrect or fails test cases.",
            "",
            "ğŸ“‹ Problem Analysis:",
            f"- Function name: {entry_point}",
            f"- Generated code length: {len(extracted_code)} characters" if extracted_code else "- No valid code could be extracted",
            "",
            "ğŸ” Common issues to check:",
            "1. Function signature matches the expected name and parameters",
            "2. Logic correctly implements the problem requirements", 
            "3. Edge cases are handled properly",
            "4. Return type matches expectations",
            "5. No syntax errors or runtime exceptions",
            ""
        ]
        
        if test_cases:
            feedback_parts.extend([
                "ğŸ§ª Test Cases to Pass:",
                f"```python\n{test_cases}\n```",
                ""
            ])
        
        if extracted_code:
            feedback_parts.extend([
                "ğŸ“ Your Generated Code:",
                f"```python\n{extracted_code}\n```",
                ""
            ])
        
        if canonical_solution:
            feedback_parts.extend([
                "âœ… Reference Solution:",
                f"```python\n{canonical_solution}\n```",
                "",
                "ğŸ’¡ Study the reference solution to understand:",
                "- The correct approach to solve this problem",
                "- How edge cases are handled",
                "- The expected function signature and behavior"
            ])
        
        return "\n".join(feedback_parts)

    def propose_new_texts(
        self,
        candidate: dict[str, str],
        reflective_dataset: dict[str, list[dict[str, Any]]],
        components_to_update: list[str],
    ) -> dict[str, str]:
        """
        GEPAì˜ ê¸°ë³¸ instruction proposal ë¡œì§ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
        
        ì‚¬ìš©ì ì •ì˜ proposal ë¡œì§ì´ í•„ìš”í•œ ê²½ìš° ì´ ë©”ì„œë“œë¥¼ ì˜¤ë²„ë¼ì´ë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        ì˜ˆ: ë‹¤ë¥¸ LLM ì‚¬ìš©, DSPy ì‹œê·¸ë‹ˆì²˜ êµ¬í˜„, ì—¬ëŸ¬ ì»´í¬ë„ŒíŠ¸ ë™ì‹œ ì—…ë°ì´íŠ¸ ë“±
        
        Args:
            candidate: í˜„ì¬ í›„ë³´ í”„ë¡œê·¸ë¨ (ì»´í¬ë„ŒíŠ¸ëª… -> ì»´í¬ë„ŒíŠ¸ í…ìŠ¤íŠ¸)
            reflective_dataset: make_reflective_datasetì—ì„œ ìƒì„±ëœ ë°˜ì„± ë°ì´í„°ì…‹
            components_to_update: ì—…ë°ì´íŠ¸í•  ì»´í¬ë„ŒíŠ¸ ëª©ë¡
            
        Returns:
            ì»´í¬ë„ŒíŠ¸ëª… -> ìƒˆë¡œìš´ ì»´í¬ë„ŒíŠ¸ í…ìŠ¤íŠ¸ ë§¤í•‘
        """
        # GEPAì˜ ê¸°ë³¸ êµ¬í˜„ì„ ì‚¬ìš© (None ë°˜í™˜ ì‹œ ê¸°ë³¸ ë¡œì§ ì‚¬ìš©)
        return {}

# -----------------------------
# Main
# -----------------------------

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # íƒ€ì„ìŠ¤íƒ¬í”„ ìƒì„± (ê²°ê³¼ íŒŒì¼ê³¼ ë™ì¼í•˜ê²Œ)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # results ë””ë ‰í† ë¦¬ ìƒì„±
    results_dir = Path("./results")
    results_dir.mkdir(exist_ok=True)
    
    # ë¡œê·¸ íŒŒì¼ ê²½ë¡œ
    log_file = results_dir / f"training_log_{timestamp}.log"
    
    print("=" * 60)
    print("ğŸš€ HumanEval í”„ë¡¬í”„íŠ¸ ìµœì í™” ì‹œì‘")
    print("=" * 60)
    print(f"ğŸ“ ë¡œê·¸ íŒŒì¼: {log_file}")
    
    # ë¡œê·¸ íŒŒì¼ì— ì‹œì‘ ë©”ì‹œì§€ ê¸°ë¡
    with open(log_file, 'w', encoding='utf-8') as f:
        f.write(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - {'=' * 60}\n")
        f.write(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - ğŸš€ HumanEval í”„ë¡¬í”„íŠ¸ ìµœì í™” ì‹œì‘\n")
        f.write(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - {'=' * 60}\n")
    
    # ë¡œê·¸ íŒŒì¼ì— ëª¨ë“  ì¶œë ¥ì„ ê¸°ë¡í•˜ëŠ” í•¨ìˆ˜
    def log_print(*args, **kwargs):
        original_print(*args, **kwargs)
        with open(log_file, 'a', encoding='utf-8') as f:
            original_print(*args, file=f, **kwargs)
    
    # ê¸°ì¡´ print í•¨ìˆ˜ë¥¼ log_printë¡œ êµì²´
    import builtins
    original_print = builtins.print
    builtins.print = log_print
    
    # Step 1: í™˜ê²½ ì„¤ì • ë° ì¸ì íŒŒì‹±
    print("\nğŸ“‹ Step 1: í™˜ê²½ ì„¤ì • ë° ì¸ì íŒŒì‹±")
    print("-" * 40)
    
    # Load environment variables from .env file
    try:
        from dotenv import load_dotenv
        load_dotenv()
        print("âœ… .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ ì™„ë£Œ")
        
    except ImportError:
        print("âš ï¸  Warning: python-dotenv not installed. Please install with 'pip install python-dotenv' or set environment variables manually.")
    
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        raise ValueError("âŒ OPENAI_API_KEY not set in environment")
    print("âœ… OpenAI API í‚¤ í™•ì¸ ì™„ë£Œ")

    parser = argparse.ArgumentParser()
    parser.add_argument("--model_name", type=str, default="gpt-4o-mini", help="Model name (e.g., gpt-3.5-turbo, gpt-4, claude-3-sonnet-20240229)")
    parser.add_argument("--humaneval_dset_name", type=str, default="openai/openai_humaneval")
    parser.add_argument("--budget", type=int, default=100, help="The budget for the optimization process.")
    parser.add_argument(
        "--reflection_lm", type=str, default="gpt-4o", help="The name of the reflection LM to use."
    )
    parser.add_argument(
        "--reflection_minibatch_size", type=int, default=5, help="The size of the minibatch for the reflection LM."
    )
    parser.add_argument(
        "--seed", type=int, default=0, help="The seed for the random number generator for reproducibility."
    )
    parser.add_argument(
        "--use_merge", action="store_true", help="Whether to use the merge strategy for candidate combination."
    )
    parser.add_argument(
        "--max_merge_invocations", type=int, default=5, help="Maximum number of merge invocations to perform."
    )
    parser.add_argument(
        "--max_dataset_size", type=int, default=None, help="Maximum dataset size to use (None for full dataset)."
    )
    parser.add_argument(
        "--train_ratio", type=float, default=0.5, help="Training set ratio (default: 0.5)."
    )
    parser.add_argument(
        "--val_ratio", type=float, default=0.3, help="Validation set ratio (default: 0.3)."
    )
    parser.add_argument(
        "--test_ratio", type=float, default=0.2, help="Test set ratio (default: 0.2)."
    )
    args = parser.parse_args()
    
    print(f"ğŸ“Š ì„¤ì •ëœ íŒŒë¼ë¯¸í„°:")
    print(f"   - ëª¨ë¸: {args.model_name}")
    print(f"   - ë°ì´í„°ì…‹: {args.humaneval_dset_name}")
    print(f"   - ì˜ˆì‚°: {args.budget}")
    print(f"   - ë°˜ì„± ëª¨ë¸: {args.reflection_lm}")
    print(f"   - ë°˜ì„± ë°°ì¹˜ í¬ê¸°: {args.reflection_minibatch_size}")
    print(f"   - ì‹œë“œ: {args.seed}")
    print(f"   - Merge ì „ëµ: {args.use_merge}")
    if args.use_merge:
        print(f"   - ìµœëŒ€ Merge íšŸìˆ˜: {args.max_merge_invocations}")
    print(f"   - ìµœëŒ€ ë°ì´í„°ì…‹ í¬ê¸°: {args.max_dataset_size or 'ì „ì²´'}")
    print(f"   - ë°ì´í„° ë¶„í•  ë¹„ìœ¨: í›ˆë ¨ {args.train_ratio:.1%}, ê²€ì¦ {args.val_ratio:.1%}, í…ŒìŠ¤íŠ¸ {args.test_ratio:.1%}")
    
    # Step 2: í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ë° ë°ì´í„°ì…‹ ë¡œë”©
    print("\nğŸ“š Step 2: í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ë° ë°ì´í„°ì…‹ ë¡œë”©")
    print("-" * 40)
    
    INSTRUCTION_PROMPT_PATH = Path(__file__).parent / "prompt-templates/instruction_prompt.txt"
    seed_instruction = INSTRUCTION_PROMPT_PATH.read_text()
    print(f"âœ… ì‹œë“œ í”„ë¡¬í”„íŠ¸ ë¡œë“œ ì™„ë£Œ: {INSTRUCTION_PROMPT_PATH}")
    print(f"   í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(seed_instruction)} ë¬¸ì")
    
    # ì‹œë“œ í”„ë¡¬í”„íŠ¸ë¥¼ ë¡œê·¸ì— ê¸°ë¡
    print("\nğŸ“ ì‹œë“œ í”„ë¡¬í”„íŠ¸:")
    print("-" * 40)
    print(seed_instruction)
    print("-" * 40)

    print(f"ğŸ“¥ ë°ì´í„°ì…‹ ë¡œë”© ì¤‘: {args.humaneval_dset_name}")
    trainset, valset, testset = init_dataset(
        humaneval_dset_name=args.humaneval_dset_name,
        max_dataset_size=args.max_dataset_size,
        train_ratio=args.train_ratio,
        val_ratio=args.val_ratio,
        test_ratio=args.test_ratio
    )
    print("âœ… ë°ì´í„°ì…‹ ë¡œë”© ì™„ë£Œ")

    # Step 3: ì–´ëŒ‘í„° ë° ë°˜ì„± ëª¨ë¸ ì„¤ì •
    print("\nğŸ”§ Step 3: ì–´ëŒ‘í„° ë° ë°˜ì„± ëª¨ë¸ ì„¤ì •")
    print("-" * 40)
    
    reflection_lm_name = args.reflection_lm
    adapter_model = args.model_name
    budget = args.budget
    reflection_minibatch_size = args.reflection_minibatch_size
    seed = args.seed

    def reflection_lm(prompt: str):
        """GEPA API ë°©ì‹ì˜ reflection language model í˜¸ì¶œ"""
        try:
            response = litellm.completion(
                model=reflection_lm_name,
                messages=[{"role": "user", "content": prompt}],
                api_key=api_key,
                timeout=30
            )
            return response.choices[0].message.content
        except Exception as e:
            # GEPA ì—”ì§„ì´ ì—ëŸ¬ë¥¼ ì²˜ë¦¬í•˜ë„ë¡ ì˜ˆì™¸ë¥¼ ë‹¤ì‹œ ë°œìƒì‹œí‚´
            raise RuntimeError(f"Reflection LM error: {e}") from e

    # GEPA í‘œì¤€ ë¡œê±° ì„¤ì •
    logger = StdOutLogger()
    
    print(f"âœ… ì–´ëŒ‘í„° ì„¤ì • ì™„ë£Œ:")
    print(f"   - í‰ê°€ ëª¨ë¸: {adapter_model}")
    print(f"   - ë°˜ì„± ëª¨ë¸: {reflection_lm_name}")
    print(f"   - ë°˜ì„± ë°°ì¹˜ í¬ê¸°: {reflection_minibatch_size}")
    print(f"   - ì‹œë“œ: {seed}")
    print(f"   - ë¡œê±°: GEPA StdOutLogger")

    # Step 4: ìµœì í™” ì‹¤í–‰
    print("\nğŸš€ Step 4: í”„ë¡¬í”„íŠ¸ ìµœì í™” ì‹¤í–‰")
    print("-" * 40)
    print(f"â±ï¸  ì˜ˆìƒ ì†Œìš” ì‹œê°„: {budget}íšŒ í‰ê°€ (ì•½ {budget // 2}ë¶„)")
    print("ğŸ”„ ìµœì í™” ì‹œì‘...")
    
    start_time = datetime.now()
    
    try:
        optimized_results = optimize(
            seed_candidate={"instruction_prompt": seed_instruction},
            trainset=trainset,
            valset=valset,
            adapter=HumanEvalAdapter(
                model=adapter_model, 
                api_key=api_key
            ),
            reflection_lm=reflection_lm,
            reflection_minibatch_size=reflection_minibatch_size,
            perfect_score=0.95,  # HumanEvalì—ì„œ í˜„ì‹¤ì ì¸ ëª©í‘œ
            skip_perfect_score=False,  # Perfect score ë‹¬ì„±í•´ë„ ê³„ì† ìµœì í™”
            candidate_selection_strategy="pareto",  # GEPA API ê¶Œì¥: Pareto frontier í™œìš©
            use_wandb=False,
            max_metric_calls=budget,
            seed=seed,
            display_progress_bar=True,
            raise_on_exception=True,  # GEPA API ê¶Œì¥: ì˜ˆì™¸ ë°œìƒ ì‹œ ì¤‘ë‹¨
            logger=logger,  # GEPA í‘œì¤€ ë¡œê±° ì‚¬ìš©
            use_merge=args.use_merge,  # GEPA API: Merge ì „ëµ ì‚¬ìš© ì—¬ë¶€
            max_merge_invocations=args.max_merge_invocations  # GEPA API: ìµœëŒ€ Merge íšŸìˆ˜
        )
        
        end_time = datetime.now()
        duration = end_time - start_time
        
        print(f"âœ… ìµœì í™” ì™„ë£Œ!")
        print(f"â±ï¸  ì´ ì†Œìš” ì‹œê°„: {duration}")
        
        # GEPAResult ê°ì²´ì˜ ì†ì„± í™•ì¸ ë° ì¶œë ¥
        print(f"ğŸ“Š ê²°ê³¼ ê°ì²´ íƒ€ì…: {type(optimized_results)}")
        print(f"ğŸ“Š ê²°ê³¼ ê°ì²´ ì†ì„±: {[attr for attr in dir(optimized_results) if not attr.startswith('_')]}")
        
        # GEPAResult ê°ì²´ì—ì„œ ìµœì¢… ì ìˆ˜ ì¶”ì¶œ
        if hasattr(optimized_results, 'val_aggregate_scores') and optimized_results.val_aggregate_scores:
            best_score = max(optimized_results.val_aggregate_scores)
            print(f"ğŸ“Š ìµœì¢… ì ìˆ˜: {best_score:.4f}")
        elif hasattr(optimized_results, 'best_score'):
            print(f"ğŸ“Š ìµœì¢… ì ìˆ˜: {optimized_results.best_score:.4f}")
        else:
            print(f"ğŸ“Š ìµœì¢… ì ìˆ˜: ì •ë³´ ì—†ìŒ")
        
        # ìµœì í™”ëœ í”„ë¡¬í”„íŠ¸ ì¶”ì¶œ ë° ì¶œë ¥
        optimized_prompt = None
        if hasattr(optimized_results, 'candidates') and optimized_results.candidates:
            # ìµœê³  ì ìˆ˜ë¥¼ ê°€ì§„ í›„ë³´ ì°¾ê¸°
            best_idx = getattr(optimized_results, 'best_idx', 0)
            if best_idx < len(optimized_results.candidates):
                best_candidate = optimized_results.candidates[best_idx]
                optimized_prompt = best_candidate.get('instruction_prompt', '')
        
        if optimized_prompt:
            print("\nğŸ“ ìµœì í™”ëœ í”„ë¡¬í”„íŠ¸:")
            print("-" * 40)
            print(optimized_prompt)
            print("-" * 40)
            
            # í”„ë¡¬í”„íŠ¸ ë³€í™” ë¶„ì„
            print("\nğŸ”„ í”„ë¡¬í”„íŠ¸ ë³€í™” ë¶„ì„:")
            print("-" * 40)
            if optimized_prompt != seed_instruction:
                print("âœ… í”„ë¡¬í”„íŠ¸ê°€ ìµœì í™”ë˜ì—ˆìŠµë‹ˆë‹¤!")
                print(f"   - ì‹œë“œ í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(seed_instruction)} ë¬¸ì")
                print(f"   - ìµœì í™”ëœ í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(optimized_prompt)} ë¬¸ì")
                print(f"   - ê¸¸ì´ ë³€í™”: {len(optimized_prompt) - len(seed_instruction):+d} ë¬¸ì")
                
                # ì£¼ìš” ë³€í™”ì  ë¶„ì„
                if len(optimized_prompt) > len(seed_instruction):
                    print("   - í”„ë¡¬í”„íŠ¸ê°€ ë” ìƒì„¸í•´ì¡ŒìŠµë‹ˆë‹¤")
                elif len(optimized_prompt) < len(seed_instruction):
                    print("   - í”„ë¡¬í”„íŠ¸ê°€ ë” ê°„ê²°í•´ì¡ŒìŠµë‹ˆë‹¤")
                else:
                    print("   - í”„ë¡¬í”„íŠ¸ ê¸¸ì´ëŠ” ë™ì¼í•˜ì§€ë§Œ ë‚´ìš©ì´ ë³€ê²½ë˜ì—ˆìŠµë‹ˆë‹¤")
            else:
                print("â„¹ï¸  í”„ë¡¬í”„íŠ¸ê°€ ë³€ê²½ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤ (ì‹œë“œ í”„ë¡¬í”„íŠ¸ê°€ ìµœì  ìƒíƒœ)")
        else:
            print("\nâš ï¸  ìµœì í™”ëœ í”„ë¡¬í”„íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

    except Exception as e:
        print(f"âŒ ìµœì í™” ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
    
    # Step 5: ê²°ê³¼ ì €ì¥
    print("\nğŸ’¾ Step 5: ê²°ê³¼ ì €ì¥")
    print("-" * 40)
    
    # results ë””ë ‰í† ë¦¬ ìƒì„±
    results_dir = Path("./results")
    results_dir.mkdir(exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_file = results_dir / f"optimized_results_{timestamp}.json"
    
    # ìƒì„¸í•œ ê²°ê³¼ ì •ë³´ë¥¼ í¬í•¨í•œ ì €ì¥
    result_data = {
        "metadata": {
            "timestamp": timestamp,
            "duration_seconds": duration.total_seconds(),
            "model_name": adapter_model,
            "reflection_lm": reflection_lm_name,
            "budget": budget,
            "seed": seed,
            "dataset_info": {
                "name": args.humaneval_dset_name,
                "train_size": len(trainset),
                "val_size": len(valset),
                "test_size": len(testset)
            }
        },
        "optimization_results": optimized_results.to_dict()
    }
    
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(result_data, f, indent=2, ensure_ascii=False)
    
    print(f"âœ… ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {output_file}")
    print(f"ğŸ“ íŒŒì¼ í¬ê¸°: {output_file.stat().st_size / 1024:.1f} KB")
    
    # ìµœì¢… ìš”ì•½
    print("\n" + "=" * 60)
    print("ğŸ‰ HumanEval í”„ë¡¬í”„íŠ¸ ìµœì í™” ì™„ë£Œ!")
    print("=" * 60)
    # ìµœì¢… ì„±ëŠ¥ ì¶œë ¥ (GEPAResult ê°ì²´ì—ì„œ ì¶”ì¶œ)
    if hasattr(optimized_results, 'val_aggregate_scores') and optimized_results.val_aggregate_scores:
        best_score = max(optimized_results.val_aggregate_scores)
        print(f"ğŸ“Š ìµœì¢… ì„±ëŠ¥: {best_score:.4f}")
    elif hasattr(optimized_results, 'best_score'):
        print(f"ğŸ“Š ìµœì¢… ì„±ëŠ¥: {optimized_results.best_score:.4f}")
    else:
        print(f"ğŸ“Š ìµœì¢… ì„±ëŠ¥: ì •ë³´ ì—†ìŒ")
    
    # ìµœì í™”ëœ í”„ë¡¬í”„íŠ¸ë¥¼ ìµœì¢… ìš”ì•½ì— í¬í•¨
    if optimized_prompt:
        print("\nğŸ“ ìµœì í™”ëœ í”„ë¡¬í”„íŠ¸:")
        print("-" * 40)
        print(optimized_prompt)
        print("-" * 40)
    
    print(f"â±ï¸  ì´ ì†Œìš” ì‹œê°„: {duration}")
    print(f"ğŸ’¾ ê²°ê³¼ íŒŒì¼: {output_file}")
    print(f"ğŸ“ ë¡œê·¸ íŒŒì¼: {log_file}")
    print("=" * 60)
    
    # ë¡œê·¸ íŒŒì¼ì— ì™„ë£Œ ë©”ì‹œì§€ ë° ìµœì í™”ëœ í”„ë¡¬í”„íŠ¸ ê¸°ë¡
    with open(log_file, 'a', encoding='utf-8') as f:
        f.write(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - â±ï¸  ì´ ì†Œìš” ì‹œê°„: {duration}\n")
        f.write(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - ğŸ’¾ ê²°ê³¼ íŒŒì¼: {output_file}\n")
        f.write(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - ğŸ“ ë¡œê·¸ íŒŒì¼: {log_file}\n")
        
        # ìµœì í™”ëœ í”„ë¡¬í”„íŠ¸ë¥¼ ë¡œê·¸ì— ê¸°ë¡
        if optimized_prompt:
            f.write(f"\nğŸ“ ìµœì í™”ëœ í”„ë¡¬í”„íŠ¸:\n")
            f.write("-" * 40 + "\n")
            f.write(f"{optimized_prompt}\n")
            f.write("-" * 40 + "\n")
        
        f.write(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - {'=' * 60}\n")
    
    # print í•¨ìˆ˜ ë³µì›
    builtins.print = original_print
    
    print(f"\nâœ… ë¡œê·¸ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {log_file}")


if __name__ == "__main__":
    main()